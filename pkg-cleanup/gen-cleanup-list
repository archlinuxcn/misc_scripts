#!/usr/bin/python3

from __future__ import annotations

from pathlib import Path
import logging
from collections import defaultdict
import time
from typing import Optional, Any, TYPE_CHECKING
import pickle
import dataclasses
if TYPE_CHECKING:
  import datetime

import requests

from myutils import restart_if_failed

from lilac2 import lilacyaml
from lilac2.packages import get_split_packages

import const

logger = logging.getLogger(__name__)

@dataclasses.dataclass
class PkgInfo:
  pkgbase: str
  dl_count: int
  stats_count: int
  stats_samples: int
  fail_count: Optional[int] = None
  fail_since: Optional[int] = None

  @property
  def stats_popularity(self) -> float:
    return 100.0 * self.stats_count / self.stats_samples

DL_COUNT_START = int(time.time() - 3 * 30 * 86400)
STATS_START = time.strftime(
  '%Y%m', time.localtime(DL_COUNT_START))
STATS_END = time.strftime('%Y%m')

s = requests.Session()

def get_dl_count(cache: dict[str, int], name: str) -> int:
  if (c := cache.get(name)) is not None:
    return c

  r = s.get(
    'https://archlinuxcn-pkgstats.imlonghao.workers.dev/',
    params = {
      'name': name,
      'timegt': str(DL_COUNT_START),
    },
  )
  j = r.json()
  c = j['count']
  cache[name] = c
  return c

def get_stats_info(cache: dict[str, tuple[int, int]], name: str) -> tuple[int, int]:
  if (d := cache.get(name)) is not None:
    return d

  r = s.get(
    f'https://pkgstats.archlinux.de/api/packages/{name}',
    params = {
      'startMonth': STATS_START,
      'endMonth': STATS_END,
    },
  )
  j = r.json()
  d = j['count'], j['samples']
  cache[name] = d
  return d

def gather_data() -> None:
  try:
    with open('cache.pickle', 'rb') as f:
      cache = pickle.load(f)
  except FileNotFoundError:
    cache = {
      'dl_count': {},
      'pkgstats': {},
    }

  try:
    gather_data_real(cache)
  finally:
    with open('cache.pickle', 'wb') as f:
      pickle.dump(cache, f)

def gather_data_real(cache: dict[Any, Any]) -> None:
  repodir = Path(const.REPODIR)
  who_maint_what = defaultdict(list)
  for pkgdir in lilacyaml.iter_pkgdir(repodir):
    y = lilacyaml.load_lilac_yaml(pkgdir)
    name = pkgdir.name
    # Accumulate counts for split packages
    packages = get_split_packages(pkgdir)
    logger.info('processing %s with packages %s...', name, packages)
    total_pkgstats_cn_count = 0
    total_pkgstats_count = 0
    pkgstats_samples = 0
    for _, pkgname in packages:
      total_pkgstats_cn_count += restart_if_failed(
        get_dl_count, 5, (cache['dl_count'], pkgname))
      cur_pkgstat_count, pkgstats_samples = restart_if_failed(
        get_stats_info, 5, (cache['pkgstats'], pkgname))
      total_pkgstats_count += cur_pkgstat_count
    info = PkgInfo(
      pkgbase = name,
      dl_count = total_pkgstats_cn_count,
      stats_count = total_pkgstats_count,
      stats_samples = pkgstats_samples,
    )
    logger.info('Queried: %s', info)
    for m in y.get('maintainers', []):
      who_maint_what[m['github']].append(info)

  save_data(who_maint_what)

def save_data(data: dict[str, list[PkgInfo]]) -> None:
  with open('data.save', 'wb') as f:
    pickle.dump(data, f)

def load_data() -> dict[str, list[PkgInfo]]:
  with open('data.save', 'rb') as f:
    return pickle.load(f)

def gen_comments() -> list[str]:
  who_maint_what = load_data()
  ret = []
  for who, pkgs in sorted(who_maint_what.items()):
    msg = [f'@{who}:']
    pkgs.sort(key=lambda x: (-x.fail_since if x.fail_since else 0, x.dl_count + x.stats_count, x.pkgbase))
    for x in pkgs:
      line = f'* [ ] {x.pkgbase} (â¬‡ï¸{x.dl_count} ðŸ“Š{x.stats_count}ðŸ”¥{x.stats_popularity:0.2f}'
      if x.fail_since is not None:
        line += f' âŒ{x.fail_count} since {x.fail_since} days'
      line += ')'
      msg.append(line)
    ret.append('\n'.join(msg))

  return ret

def post_comments(token: str, issue: int) -> None:
  from github import GitHub

  comments = gen_comments()
  gh = GitHub(token)

  for c in comments:
    r = gh.add_issue_comment(
      'archlinuxcn/repo',
      issue,
      c,
    )
    r.text
    # https://developer.github.com/v3/guides/best-practices-for-integrators/#dealing-with-abuse-rate-limits
    time.sleep(1)

def annotate_build_results() -> None:
  import datetime

  data = load_data()
  pkgs = list(set(x.pkgbase for v in data.values() for x in v))
  now = datetime.datetime.now().astimezone()
  pkg_build_status = load_build_status_count(pkgs, now)

  for v in data.values():
    for x in v:
      if cs := pkg_build_status.get(x.pkgbase):
        x.fail_count, x.fail_since = cs

  save_data(data)

def load_build_status_count(
  pkgs: list[str],
  now: datetime.datetime,
) -> dict[str, tuple[int, Optional[int]]]:
  from itertools import groupby, takewhile
  import psycopg2

  logging.info('loading data from Postgres...')
  conn = psycopg2.connect('')
  with conn:
    cursor = conn.cursor()
    cursor.execute('select pkgbase, ts, result from lilac.pkglog where pkgbase = any(%s) order by pkgbase, ts desc', (pkgs,))
    rows = cursor.fetchall()
  logging.info('loaded data from Postgres.')

  pkg_to_result = {}
  for k, g in groupby(rows, key=lambda row: row[0]):
    rs = list(takewhile(lambda row: row[2] == 'failed', g))
    if rs:
      days = (now - rs[-1][1]).days
    else:
      days = None
    pkg_to_result[k] = len(rs), days

  return pkg_to_result

def print_data() -> None:
  comments = gen_comments()
  for c in comments:
    print(c, end='\n\n')

def convert_data():
  data = load_data()

  for k, v in data.items():
    v[:] = [
      PkgInfo(
        pkgbase = x.pkgbase,
        dl_count = x.dl_count,
        stats_count = x.stats_count,
        stats_samples = x.stats_samples,
      ) for x in v]

  save_data(data)

if __name__ == '__main__':
  from nicelogger import enable_pretty_logging
  enable_pretty_logging('DEBUG')
  # gather_data()
  # convert_data()
  # annotate_build_results()
  print_data()
  # post_comments(const.GITHUB_TOKEN, const.ISSUE_NR)
